{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pipeline for https://www.kaggle.com/c/feedback-prize-2021\n",
    "* Run POS tagging on the text\n",
    "* Label the text according to the BIO tagging convention per dict below (mapping below taken from https://www.kaggle.com/abhishek/two-longformers-are-better-than-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "#target_id_map = {k: str(v) for k, v in ra}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rajiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\rajiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "tagmap = {k:v for v, k in enumerate(sorted(tagdict.keys())) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lead 1', 'Position 1', 'Evidence 1', 'Evidence 2', 'Claim 1',\n",
       "       'Evidence 3', 'Evidence 4', 'Claim 2', 'Evidence 5',\n",
       "       'Concluding Statement 1', 'Counterclaim 1', 'Rebuttal 1',\n",
       "       'Claim 3', 'Claim 4', 'Claim 5', 'Claim 6', 'Claim 7',\n",
       "       'Counterclaim 2', 'Rebuttal 2', 'Counterclaim 3', 'Rebuttal 3',\n",
       "       'Evidence 6', 'Lead 2', 'Counterclaim 4', 'Counterclaim 5',\n",
       "       'Counterclaim 6', 'Evidence 7', 'Claim 8', 'Evidence 8',\n",
       "       'Concluding Statement 2', 'Rebuttal 4', 'Rebuttal 5', 'Claim 9',\n",
       "       'Position 2', 'Claim 10', 'Claim 11', 'Claim 12', 'Evidence 9',\n",
       "       'Concluding Statement 3', 'Concluding Statement 4', 'Evidence 10',\n",
       "       'Evidence 11', 'Rebuttal 6', 'Evidence 12'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/train.csv\")\n",
    "df.discourse_type_num.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append 2 columns for (a) POS tags and (b) BIO tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a lookup for filename, word index => discource_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dict = {}\n",
    "\n",
    "for _, row in df.T.to_dict().items():\n",
    "    f_name = row['id']\n",
    "    d_type = row['discourse_type_num']\n",
    "    p_string = row['predictionstring']\n",
    "\n",
    "    for w_idx in p_string.split():\n",
    "        w_idx = int(w_idx.strip())\n",
    "        l_dict[(str(f_name), w_idx)] = d_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in all files in training set, generate POS tags. Generate a dataframe with cols = `id, discourse_text, discourse_type, predictionstring, POS tags, BIO tags`\n",
    "* go through each line in labeled dataset, read in the file\n",
    "* run POS tags using NLTK\n",
    "* for each file read in, split to words and check if word index exists in labeled data\n",
    "* create a list of dicts with dict keys = columnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [02:55<00:00, 88.97it/s] \n"
     ]
    }
   ],
   "source": [
    "e_data = []\n",
    "for f_name in tqdm(df.id.unique()):\n",
    "    f_name = str(f_name)\n",
    "    f_rows = []\n",
    "    with open(f\"../../data/train/{f_name}.txt\") as t_file:\n",
    "        l_class = None\n",
    "        l_disc_type = None\n",
    "        f_content = t_file.read().split()\n",
    "        pos_tags = nltk.pos_tag(f_content)\n",
    "        pos_tags = [tagmap.get(pt[1], -1) for pt in pos_tags]\n",
    "        c_text, c_pred_str, c_pos, c_bio = [], [], [], []\n",
    "        for idx, token in enumerate(f_content):\n",
    "            c_class = l_dict.get((f_name, idx), \"O\")\n",
    "            if c_class == \"O\":\n",
    "                c_disc_type = \"O\"\n",
    "            else:\n",
    "                c_disc_type = \" \".join(c_class.split(\" \")[:-1])\n",
    "            if l_class is None: \n",
    "                tmp_c = c_class\n",
    "                if c_class != \"O\":\n",
    "                    tmp_c = \"B-\"+c_disc_type\n",
    "                c_text, c_pred_str, c_pos, c_bio = [token], [idx], [pos_tags[idx]], [target_id_map[tmp_c]]\n",
    "            elif c_class != l_class:\n",
    "                c_row = {\"id\": f_name, \"discourse_type\": l_disc_type, \"discourse_text\": c_text, \"predictionstring\":c_pred_str, \"pos_tags\":c_pos, \"bio_tags\": c_bio}\n",
    "                f_rows.append(c_row)\n",
    "                if c_class != \"O\":\n",
    "                    c_text, c_pred_str, c_pos, c_bio = [token], [idx], [pos_tags[idx]], [target_id_map[\"B-\"+c_disc_type]]\n",
    "                else:\n",
    "                    c_text, c_pred_str, c_pos, c_bio = [token], [idx], [pos_tags[idx]], [target_id_map[c_disc_type]]\n",
    "            else:\n",
    "                c_text.append(token)\n",
    "                c_pred_str.append(idx)\n",
    "                c_pos.append(pos_tags[idx])\n",
    "                tmp_c = c_class\n",
    "                if c_class != \"O\":\n",
    "                    tmp_c = \"I-\"+c_disc_type\n",
    "                c_bio.append(target_id_map[tmp_c])\n",
    "            l_class = c_class\n",
    "            l_disc_type = c_disc_type\n",
    "        # handle last row\n",
    "        c_row = {\"id\": f_name, \"discourse_type\": l_class, \"discourse_text\": c_text, \"predictionstring\":c_pred_str, \"pos_tags\":c_pos, \"bio_tags\": c_bio}\n",
    "        f_rows.append(c_row)\n",
    "    e_data.extend(f_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e_data = [{\"id\": row[\"id\"], \"discourse_text\": \" \".join(row[\"discourse_text\"]), \"predictionstring\": \" \".join(row[\"predictionstring\"]), \"pos_tags\": \" \".join(row[\"pos_tags\"])} for row in e_data]\n",
    "ldata_df = pd.DataFrame(e_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata_df[ldata_df[\"id\"]==\"4C471936CD75\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"id\"]==\"4C471936CD75\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata_df.to_csv(\"../../data/train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!del /f ..\\..\\data\\train\\*.gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_file = e_data[0]['id']\n",
    "if glob.glob(\"../../data/train/*.gt\"):\n",
    "    print(\"labeled files exist. Delete prior to proceeding\")\n",
    "    \n",
    "for row in e_data:\n",
    "    f_name = row['id'] + \".gt\"\n",
    "\n",
    "    with open(f\"../../data/train/{f_name}\", \"a\") as f:\n",
    "        f.write(\"\".join([str(x).__add__(\" \") for x in row[\"bio_tags\"]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e3f9f673fbe7eac57a16625d4e046211b31da8729f242e61a467ea15576b269"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
