{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import os, sys\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertModel, AdamW, BertConfig,BertTokenizer,BertPreTrainedModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import datetime, time\n",
    "from collections import defaultdict\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"C:/Users/rajiv/dev/evalwriting/\"\n",
    "INPUT_DIR = BASE_DIR+\"inputs/\"\n",
    "RESULT_DIR = BASE_DIR+\"result/\"\n",
    "TBOARD_LOG_DIR = BASE_DIR+\"tboard/\"\n",
    "LOG_DIR = BASE_DIR+\"logs/\"\n",
    "CP_DIR = BASE_DIR+\"checkpoints/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = LOG_DIR+datetime.datetime.now().strftime(\"%Y%m%d.%H.%M\")+\".txt\"\n",
    "logging.basicConfig(filename=log_file, level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiSentenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = BertModel(config)\n",
    "        # Initialize model weights (inherited function).\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        # the below returns a tuple. First element in the tuple is last hidden state. Second element in tuple is pooler output\n",
    "        self.result = self.model(input_ids=x, attention_mask =mask, token_type_ids=segs)\n",
    "        top_vec = self.result[0]\n",
    "        return top_vec\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 8)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, mask_cls):\n",
    "#        h = self.linear2(self.tanh(self.linear1(x)))\n",
    "        h = self.linear1(x)\n",
    "        h = h.squeeze(-1)\n",
    "        sent_scores = self.softmax(h) * mask_cls.unsqueeze(2).repeat(1,1,8).float()\n",
    "        return sent_scores, x, h\n",
    "\n",
    "\n",
    "\n",
    "class PassageClassifier(nn.Module):\n",
    "    def __init__(self, args=None, num_hidden = 768, load_pretrained_bert = True, bert_config = None):\n",
    "        super(PassageClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.bert = BertForMultiSentenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "#        if (args.encoder == 'classifier'):\n",
    "        self.encoder = Classifier(num_hidden)\n",
    "        logger.debug(\"dropout = \"+str(nn.Dropout(self.bert.config.hidden_dropout_prob)))\n",
    "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        for p in self.encoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def load_cp(self, state):\n",
    "        self.load_state_dict(state, strict=True)\n",
    "\n",
    "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
    "        top_vec = self.bert(x, segs, mask)\n",
    "#        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "#        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sents_vec = top_vec.gather(1, clss.unsqueeze(-1).expand(-1, -1, 768))\n",
    "        sents_vec = self.dropout(sents_vec)\n",
    "#        cls_outs = sents_vec.clone().detach().cpu().numpy()\n",
    "#        print(\">>> \", cls_outs.shape, np.sum(cls_outs, axis=1)[:, :20])\n",
    "\n",
    "        sent_scores, x, h = self.encoder(sents_vec, mask_cls)\n",
    "        sent_scores = sent_scores.squeeze(-1)\n",
    "        return sent_scores, mask_cls, x, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'clss', 'id', 'labels', 'mask_cls', 'segs', 'src', 'wends', 'wstarts'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "train_d, val_d = dict(), dict()\n",
    "\n",
    "for d_type, dset in ((\"train\", train_d), (\"validation\", val_d)):\n",
    "    for inp_file in glob.glob(INPUT_DIR+\"*\"+d_type+\"*\"):\n",
    "        f_name = inp_file.split(\"\\\\\")[-1]\n",
    "        namelist = f_name.split(\"_\")[:-1]\n",
    "        tag = \"_\".join(namelist)\n",
    "        file_d = torch.load(inp_file)\n",
    "        #dset[tag] = file_d [:int(len(file_d)*.1)] #TODO temporary\n",
    "        dset[tag] = file_d\n",
    "        #dset = TensorDataset(train_d[\"fileidx\"], train_d[\"idx\"], train_d[\"src\"],train_d[\"labels\"], train_d[\"segs\"], train_d[\"clss\"], train_d[\"attn\"], train_d[\"mask_cls\"])\n",
    "\n",
    "train_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in (train_d, val_d):\n",
    "    for k, v in dset.items():\n",
    "        dset[k] = dset[k][:20]\n",
    "\n",
    "train_dataset = TensorDataset(train_d[\"id\"], train_d[\"src\"],  train_d[\"segs\"], train_d[\"clss\"], train_d[\"attn\"], train_d[\"mask_cls\"], train_d[\"wstarts\"], train_d[\"wends\"], train_d[\"labels\"])\n",
    "val_dataset = TensorDataset(val_d[\"id\"], val_d[\"src\"],  val_d[\"segs\"], val_d[\"clss\"], val_d[\"attn\"], val_d[\"mask_cls\"], val_d[\"wstarts\"], val_d[\"wends\"], val_d[\"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device('cuda:0'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for k, v in train_d.items():\n",
    "    del v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "with torch.cuda.device('cuda:0'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PassageClassifier()\n",
    "torch.cuda.empty_cache()\n",
    "_= model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.model.embeddings.word_embeddings.weight            (30522, 768)\n",
      "bert.model.embeddings.position_embeddings.weight          (512, 768)\n",
      "bert.model.embeddings.token_type_embeddings.weight          (2, 768)\n",
      "bert.model.embeddings.LayerNorm.weight                        (768,)\n",
      "bert.model.embeddings.LayerNorm.bias                          (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.model.encoder.layer.0.attention.self.query.weight    (768, 768)\n",
      "bert.model.encoder.layer.0.attention.self.query.bias          (768,)\n",
      "bert.model.encoder.layer.0.attention.self.key.weight      (768, 768)\n",
      "bert.model.encoder.layer.0.attention.self.key.bias            (768,)\n",
      "bert.model.encoder.layer.0.attention.self.value.weight    (768, 768)\n",
      "bert.model.encoder.layer.0.attention.self.value.bias          (768,)\n",
      "bert.model.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "bert.model.encoder.layer.0.attention.output.dense.bias        (768,)\n",
      "bert.model.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "bert.model.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "bert.model.encoder.layer.0.intermediate.dense.weight     (3072, 768)\n",
      "bert.model.encoder.layer.0.intermediate.dense.bias           (3072,)\n",
      "bert.model.encoder.layer.0.output.dense.weight           (768, 3072)\n",
      "bert.model.encoder.layer.0.output.dense.bias                  (768,)\n",
      "bert.model.encoder.layer.0.output.LayerNorm.weight            (768,)\n",
      "bert.model.encoder.layer.0.output.LayerNorm.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.model.pooler.dense.weight                            (768, 768)\n",
      "bert.model.pooler.dense.bias                                  (768,)\n",
      "encoder.linear1.weight                                      (8, 768)\n",
      "encoder.linear1.bias                                            (8,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set all layers to train\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs =10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = int(total_steps * .2),\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "loss_c = torch.nn.BCELoss(reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging initializer\n",
      "C:/Users/rajiv/dev/evalwriting/logs/20220119.07.26.txt\n"
     ]
    }
   ],
   "source": [
    "def save_cp(model_state, optimizer_state, scheduler, epoch, step=0):\n",
    "    cp = {'model_state': model_state, 'optimizer_state':optimizer_state, 'scheduler': scheduler}\n",
    "    filename = datetime.datetime.now().strftime(\"%Y%m%d.%H.\")+str(epoch)+\"_\"+str(step)+\".pth.tar\"\n",
    "    torch.save(cp, CP_DIR+filename )\n",
    "\n",
    "def load_cp(filename):\n",
    "    cp = torch.load(filename)\n",
    "    return cp['model_state'], cp['optimizer_state'], cp['scheduler']\n",
    "\n",
    "logger.debug(\"logging initializer\")\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = True\n",
    "# For each epoch...\n",
    "for epoch_i in range(epochs):\n",
    "    if EVAL:\n",
    "        break\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            logger.debug('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        id, src, segs, clss, attn, mask_cls, wstarts, wends, labels = batch\n",
    "\n",
    "        src, segs, clss, attn, mask_cls, wstarts, wends, labels = src.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device), wstarts.to(device), wends.to(device),labels.to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
    "        loss = loss_c(probs, labels.float())\n",
    "        loss = (loss * mask_cls.unsqueeze(2).repeat(1,1,8).float()).sum()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        if step % 5000 == 0 and step != 0:\n",
    "            logger.debug(\"saving checkpoint\"+ str(epoch_i))\n",
    "            save_cp(model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), epoch_i, step)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # save checkpoint\n",
    "    #if (epoch_i+1 )%5 == 0:\n",
    "    if True:\n",
    "        logger.debug(\"saving checkpoint\"+ str(epoch_i))\n",
    "        save_cp(model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), epoch_i)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    logger.debug(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    logger.debug(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    step = -1\n",
    "    filesets = list()\n",
    "    auclist = []\n",
    "    accuracy = []\n",
    "    for batch in validation_dataloader:\n",
    "        step += 1\n",
    "        id, src, segs, clss, attn, mask_cls, wstarts, wends, labels = batch\n",
    "        src, segs, clss, attn, mask_cls, wstarts, wends, labels = src.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device), wstarts.to(device), wends.to(device),labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
    "            loss = loss_c(probs, labels.float())\n",
    "            loss = (loss * mask_cls.unsqueeze(2).repeat(1,1,8).float()).sum()\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
    "    #print(\"Average AUC\", np.mean(np.array([x for x in auclist])) )\n",
    "    logger.debug(\"  Average validation loss: {0:.2f}\".format(avg_eval_loss))\n",
    "    logger.debug(\"  Validation epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoints if provided\n",
    "#cp_file = BASE_DIR+\"checkpoints/\"+\"20220115.18.5_0.pth.tar\"\n",
    "cp_file = BASE_DIR+\"checkpoints/\"+\"20220118.13.71_0.pth.tar\"\n",
    "if cp_file:\n",
    "    m_state, o_state, s_state = load_cp(cp_file)\n",
    "    model.load_cp(m_state)\n",
    "    optimizer.load_state_dict(o_state)\n",
    "    scheduler.load_state_dict(s_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset, construct the expected outputs\n",
    "# construct the predicted outputs from bert call\n",
    "# run the metrics\n",
    "\n",
    "groundtruth = {}\n",
    "disc_types = {'Claim': 0,\n",
    " 'Concluding Statement': 1,\n",
    " 'Counterclaim': 2,\n",
    " 'Evidence': 3,\n",
    " 'Lead': 4,\n",
    " 'Position': 5,\n",
    " 'Rebuttal': 6,\n",
    " 'None': 7}\n",
    "disc_types = {v:k for k, v in disc_types.items()}\n",
    "\n",
    "file_id_map = pd.read_csv(BASE_DIR+\"inputs/f_mapping.csv\")\n",
    "file_id_map = {row.to_dict()['f_id']:row.to_dict()['id'] for k, row in file_id_map.iterrows()}\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "pred_by_file = defaultdict(list)\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    id, src, segs, clss, attn, mask_cls, wstarts, wends, labels = batch\n",
    "    src, segs, clss, attn, mask_cls, wstarts, wends, labels = src.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device), wstarts.to(device), wends.to(device),labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
    "        pred_labs = torch.argmax(probs, dim=2)\n",
    "\n",
    "        for r in range(pred_labs.size(dim=0)):\n",
    "            id_lookup = int(id[r])\n",
    "            # read ground truth\n",
    "            f_name = file_id_map[id_lookup]\n",
    "            gt_df = train_df [train_df.id == f_name]\n",
    "            gt_df = gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n",
    "            gt_df.to_csv(BASE_DIR+\"/result/\"+f_name+\".gt.csv\")\n",
    "            groundtruth[f_name] = gt_df\n",
    "            ws, we = wstarts[r].tolist(), wends[r].tolist()\n",
    "            preds = pred_labs[r, :]\n",
    "            for s_num in range(int(mask_cls[r, :].sum())):\n",
    "                p_dict = {}\n",
    "                p_dict[\"id\"] = f_name\n",
    "                p_dict[\"discourse_type\"] = disc_types[int(preds[s_num])]\n",
    "                ws_i, we_i = int(ws[s_num]), int(we[s_num])\n",
    "                p_dict[\"predictionstring\"] = \" \".join([str(x) for x in range(ws_i, we_i+1)])\n",
    "                if p_dict[\"predictionstring\"].strip() == \"\":\n",
    "                    p_dict[\"predictionstring\"] = \"7\"\n",
    "                pred_by_file[f_name].append(p_dict)\n",
    "\n",
    "        loss = loss_c(probs, labels.float())\n",
    "        loss = (loss * mask_cls.unsqueeze(2).repeat(1,1,8).float()).sum()\n",
    "        #total_eval_loss += loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_by_file_cleaned = defaultdict(list)\n",
    "for f_name, preds in pred_by_file.items():\n",
    "    merged_preds = []\n",
    "    for i, p in enumerate(preds):\n",
    "        if p[\"discourse_type\"] == \"None\":\n",
    "            continue\n",
    "\n",
    "        if i != 0:\n",
    "            if merged_preds[-1][\"id\"] == p[\"id\"] and merged_preds[-1][\"discourse_type\"] == p[\"discourse_type\"]:\n",
    "                w_idxs_a = [int(x) for x in merged_preds[-1][\"predictionstring\"].split(\" \")]\n",
    "                w_idxs_b = [int(x) for x in p[\"predictionstring\"].split(\" \")]\n",
    "                if w_idxs_a[-1] == w_idxs_b[0]-1:\n",
    "                    merged_preds[-1]['predictionstring'] = merged_preds[-1]['predictionstring'] + \" \" + p[\"predictionstring\"]\n",
    "                else:\n",
    "                    merged_preds.append(p)\n",
    "            else:\n",
    "                merged_preds.append(p)\n",
    "        else:\n",
    "            merged_preds.append(p)\n",
    "        \n",
    "    pred_by_file_cleaned[f_name].extend(merged_preds)\n",
    "\n",
    "for f_name, sents in pred_by_file_cleaned.items():\n",
    "    pred_df = pd.DataFrame(sents)\n",
    "    pred_df.to_csv(BASE_DIR+\"/result/\"+f_name+\".pred.csv\")\n",
    "    pred_by_file_cleaned[f_name] = pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below copied from https://www.kaggle.com/robikscube/student-writing-competition-twitch-stream?scriptVersionId=83303421&cellId=31\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(\" \"))\n",
    "    set_gt = set(row.predictionstring_gt.split(\" \"))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter / len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp_micro(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = (\n",
    "        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    pred_df[\"pred_id\"] = pred_df.index\n",
    "    gt_df[\"gt_id\"] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(\n",
    "        gt_df,\n",
    "        left_on=[\"id\", \"class\"],\n",
    "        right_on=[\"id\", \"discourse_type\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_pred\", \"_gt\"),\n",
    "    )\n",
    "    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n",
    "    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n",
    "\n",
    "    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n",
    "    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n",
    "    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n",
    "    tp_pred_ids = (\n",
    "        joined.query(\"potential_TP\")\n",
    "        .sort_values(\"max_overlap\", ascending=False)\n",
    "        .groupby([\"id\", \"predictionstring_gt\"])\n",
    "        .first()[\"pred_id\"]\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n",
    "    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    # calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n",
    "    return my_f1_score\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n",
    "        pred_subset = (\n",
    "            pred_df.loc[pred_df[\"class\"] == discourse_type]\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6392498110355253\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for k, gt_df in groundtruth.items():\n",
    "    pred_df = pred_by_file_cleaned[k]\n",
    "    pred_df = pred_df.rename(columns={\"discourse_type\":\"class\"})\n",
    "    #print(pred_df[[\"id\", \"class\", \"predictionstring\"]])\n",
    "    scores.append(score_feedback_comp(pred_df, gt_df))\n",
    "\n",
    "print(np.mean(np.array(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f90468882d9bbd5a69940b2e17f7aca80906dbdaa30df8068962c08e6b1e26a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
